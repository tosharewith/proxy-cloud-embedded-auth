# Multi-Provider AI Proxy Architecture

## ğŸ¯ Vision

Transform the Bedrock IAM Proxy into a **unified AI gateway** that:

1. **Multi-Provider Support**: Transparent proxy to AWS Bedrock, Azure AI, OpenAI, Anthropic Direct, Google Vertex AI
2. **OpenAI Compatibility**: Expose OpenAI-compatible API endpoints that route to any backend
3. **Security-First**: Maintain existing multi-layer authentication architecture
4. **Developer-Friendly**: One proxy, any AI provider, consistent authentication

---

## ğŸ—ï¸ Three-Layer Architecture

```mermaid
flowchart TB
    subgraph Layer1["LAYER 1: CLIENT INTERFACE"]
        OPENAI_CHAT["/v1/chat/completions"]
        OPENAI_COMP["/v1/completions"]
        OPENAI_MODELS["/v1/models"]
        OPENAI_EMBED["/v1/embeddings"]

        NATIVE_BEDROCK["/transparent/bedrock/*"]
        NATIVE_AZURE["/transparent/azure/*"]
        NATIVE_OPENAI["/transparent/openai/*"]
        NATIVE_ANTHROPIC["/transparent/anthropic/*"]
        NATIVE_VERTEX["/transparent/vertex/*"]
    end

    subgraph Layer2["LAYER 2: AUTH & ROUTING"]
        AUTH[Authentication Layer]
        ROUTER[Router and Translator]
    end

    subgraph Layer3["LAYER 3: PROVIDER HANDLERS"]
        H_BEDROCK[Bedrock Handler]
        H_AZURE[Azure Handler]
        H_OPENAI[OpenAI Handler]
        H_ANTHROPIC[Anthropic Handler]
        H_VERTEX[Vertex Handler]
        H_IBM[IBM Handler]
        H_ORACLE[Oracle Handler]
    end

    subgraph Providers["AI PROVIDERS"]
        P_BEDROCK[AWS Bedrock]
        P_AZURE[Azure AI]
        P_OPENAI[OpenAI API]
        P_ANTHROPIC[Anthropic API]
        P_VERTEX[Google Vertex]
        P_IBM[IBM watsonx.ai]
        P_ORACLE[Oracle Cloud AI]
    end

    OPENAI_CHAT --> AUTH
    OPENAI_COMP --> AUTH
    OPENAI_MODELS --> AUTH
    OPENAI_EMBED --> AUTH
    NATIVE_BEDROCK --> AUTH
    NATIVE_AZURE --> AUTH
    NATIVE_OPENAI --> AUTH
    NATIVE_ANTHROPIC --> AUTH
    NATIVE_VERTEX --> AUTH

    AUTH --> ROUTER

    ROUTER --> H_BEDROCK
    ROUTER --> H_AZURE
    ROUTER --> H_OPENAI
    ROUTER --> H_ANTHROPIC
    ROUTER --> H_VERTEX
    ROUTER --> H_IBM
    ROUTER --> H_ORACLE

    H_BEDROCK --> P_BEDROCK
    H_AZURE --> P_AZURE
    H_OPENAI --> P_OPENAI
    H_ANTHROPIC --> P_ANTHROPIC
    H_VERTEX --> P_VERTEX
    H_IBM --> P_IBM
    H_ORACLE --> P_ORACLE

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#e8f5e9
    style Providers fill:#f3e5f5
```

---

## ğŸ“ Project Structure

```
bedrock-proxy-iam/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go                    # Entry point with multi-provider routing
â”‚
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ auth/                          # Authentication layer
â”‚   â”‚   â”œâ”€â”€ apikey_db.go              # User API key management (existing)
â”‚   â”‚   â”œâ”€â”€ totp.go                   # 2FA support (existing)
â”‚   â”‚   â”œâ”€â”€ aws_signer.go             # AWS SigV4 signing (existing)
â”‚   â”‚   â”œâ”€â”€ azure_signer.go           # NEW: Azure AD token + API key
â”‚   â”‚   â””â”€â”€ gcp_signer.go             # NEW: GCP service account JWT
â”‚   â”‚
â”‚   â”œâ”€â”€ middleware/                    # HTTP middleware
â”‚   â”‚   â”œâ”€â”€ auth_enhanced.go          # User authentication (existing)
â”‚   â”‚   â”œâ”€â”€ logging.go                # Request logging (existing)
â”‚   â”‚   â”œâ”€â”€ metrics.go                # Prometheus metrics (existing)
â”‚   â”‚   â””â”€â”€ security.go               # Security headers (existing)
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/                     # NEW: Provider implementations
â”‚   â”‚   â”œâ”€â”€ interface.go              # Provider interface definition
â”‚   â”‚   â”œâ”€â”€ bedrock/
â”‚   â”‚   â”‚   â”œâ”€â”€ bedrock.go            # AWS Bedrock handler (refactored from proxy/)
â”‚   â”‚   â”‚   â””â”€â”€ models.go             # Model definitions
â”‚   â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â”‚   â”œâ”€â”€ azure.go              # Azure AI handler
â”‚   â”‚   â”‚   â””â”€â”€ models.go             # Model mappings
â”‚   â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.go             # OpenAI handler
â”‚   â”‚   â”‚   â””â”€â”€ models.go             # Model list
â”‚   â”‚   â”œâ”€â”€ anthropic/
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic.go          # Anthropic Direct API handler
â”‚   â”‚   â”‚   â””â”€â”€ models.go             # Model definitions
â”‚   â”‚   â””â”€â”€ vertex/
â”‚   â”‚       â”œâ”€â”€ vertex.go             # Google Vertex AI handler
â”‚   â”‚       â””â”€â”€ models.go             # Model mappings
â”‚   â”‚
â”‚   â”œâ”€â”€ translator/                    # NEW: OpenAI API translation layer
â”‚   â”‚   â”œâ”€â”€ openai_to_bedrock.go      # OpenAI â†’ Bedrock format
â”‚   â”‚   â”œâ”€â”€ openai_to_azure.go        # OpenAI â†’ Azure format
â”‚   â”‚   â”œâ”€â”€ openai_to_anthropic.go    # OpenAI â†’ Anthropic format
â”‚   â”‚   â”œâ”€â”€ openai_to_vertex.go       # OpenAI â†’ Vertex format
â”‚   â”‚   â”œâ”€â”€ response_normalizer.go    # Normalize all responses to OpenAI format
â”‚   â”‚   â””â”€â”€ streaming.go              # Streaming response handler
â”‚   â”‚
â”‚   â”œâ”€â”€ router/                        # NEW: Smart routing logic
â”‚   â”‚   â”œâ”€â”€ model_mapper.go           # Model name â†’ Provider mapping
â”‚   â”‚   â”œâ”€â”€ config.go                 # Route configuration
â”‚   â”‚   â””â”€â”€ router.go                 # Route selection logic
â”‚   â”‚
â”‚   â””â”€â”€ health/                        # Health checks (existing)
â”‚       â””â”€â”€ checker.go
â”‚
â”œâ”€â”€ pkg/
â”‚   â””â”€â”€ metrics/                       # Prometheus metrics (existing)
â”‚       â””â”€â”€ metrics.go
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model-mapping.yaml            # NEW: Model to provider mappings
â”‚   â””â”€â”€ provider-config.yaml          # NEW: Provider-specific configurations
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md               # Original architecture (existing)
    â”œâ”€â”€ MULTI-PROVIDER-ARCHITECTURE.md # This document
    â”œâ”€â”€ PROVIDER-GUIDE.md             # NEW: Guide for each provider
    â””â”€â”€ OPENAI-COMPATIBILITY.md       # NEW: OpenAI API compatibility matrix
```

---

## ğŸ”Œ Provider Interface

All providers implement a common interface:

```go
// internal/providers/interface.go
package providers

import (
	"context"
	"io"
	"net/http"
)

// Provider defines the interface that all AI providers must implement
type Provider interface {
	// Name returns the provider identifier (bedrock, azure, openai, anthropic, vertex)
	Name() string

	// HealthCheck verifies the provider is accessible
	HealthCheck(ctx context.Context) error

	// Invoke sends a request to the provider's native API
	// Returns response body, status code, headers, and error
	Invoke(ctx context.Context, request *ProviderRequest) (*ProviderResponse, error)

	// InvokeStreaming handles streaming responses (for chat completions)
	InvokeStreaming(ctx context.Context, request *ProviderRequest) (io.ReadCloser, error)

	// ListModels returns available models for this provider
	ListModels(ctx context.Context) ([]Model, error)

	// GetModelInfo returns details about a specific model
	GetModelInfo(ctx context.Context, modelID string) (*Model, error)
}

// ProviderRequest wraps the provider-specific request
type ProviderRequest struct {
	Method      string            // HTTP method (POST, GET, etc.)
	Path        string            // API endpoint path
	Headers     map[string]string // HTTP headers
	Body        []byte            // Request body (JSON)
	QueryParams map[string]string // URL query parameters
	Metadata    map[string]any    // Additional context (user info, etc.)
}

// ProviderResponse wraps the provider's response
type ProviderResponse struct {
	StatusCode int               // HTTP status code
	Headers    map[string]string // Response headers
	Body       []byte            // Response body (JSON)
	Metadata   map[string]any    // Additional metadata (latency, tokens, etc.)
}

// Model represents an AI model
type Model struct {
	ID          string   // Model identifier (e.g., "gpt-4", "claude-3-sonnet")
	Provider    string   // Provider name
	Name        string   // Human-readable name
	Description string   // Model description
	Capabilities []string // Features: "chat", "completion", "embeddings", "streaming"
	ContextWindow int     // Max context length
	InputPrice   float64  // Price per 1M input tokens (USD)
	OutputPrice  float64  // Price per 1M output tokens (USD)
}
```

---

## ğŸŒ API Endpoints

### 1. Native Provider APIs (Transparent Proxy)

Direct access to each provider's native API:

```bash
# AWS Bedrock
POST /providers/bedrock/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke
POST /providers/bedrock/model/{model-id}/invoke-with-response-stream

# Azure AI
POST /providers/azure/openai/deployments/{deployment-id}/chat/completions

# OpenAI
POST /providers/openai/v1/chat/completions
POST /providers/openai/v1/completions

# Anthropic Direct
POST /providers/anthropic/v1/messages
POST /providers/anthropic/v1/complete

# Google Vertex AI
POST /providers/vertex/v1/projects/{project}/locations/{location}/publishers/anthropic/models/{model}:streamRawPredict
```

**Use cases:**
- Advanced users who need provider-specific features
- Testing different providers
- Debugging
- Provider-specific optimizations

### 2. OpenAI-Compatible API (Unified Interface)

Standard OpenAI API format routes to any provider:

```bash
# Chat completions (maps to any provider)
POST /v1/chat/completions
{
  "model": "gpt-4",  # or "claude-3-sonnet", "gemini-pro", etc.
  "messages": [{"role": "user", "content": "Hello!"}],
  "stream": true
}

# Text completions (legacy)
POST /v1/completions

# Embeddings
POST /v1/embeddings

# Models list
GET /v1/models

# Model details
GET /v1/models/{model-id}
```

**Use cases:**
- Drop-in replacement for OpenAI
- Easy migration from OpenAI to other providers
- Framework compatibility (LangChain, LlamaIndex, etc.)
- A/B testing across providers

---

## ğŸ—ºï¸ Model Mapping Configuration

```yaml
# configs/model-mapping.yaml

# OpenAI model names map to provider-specific models
model_mappings:
  # GPT-4 family â†’ Multiple providers
  gpt-4:
    default_provider: openai
    providers:
      openai:
        model: gpt-4-0125-preview
        api_version: "2024-02-01"
      azure:
        deployment: gpt-4-deployment
        api_version: "2024-02-15-preview"

  gpt-4-turbo:
    default_provider: openai
    providers:
      openai:
        model: gpt-4-turbo-preview

  # Claude family â†’ Anthropic or Bedrock
  claude-3-opus:
    default_provider: bedrock
    providers:
      bedrock:
        model: anthropic.claude-3-opus-20240229-v1:0
      anthropic:
        model: claude-3-opus-20240229

  claude-3-sonnet:
    default_provider: bedrock
    providers:
      bedrock:
        model: anthropic.claude-3-sonnet-20240229-v1:0
      anthropic:
        model: claude-3-sonnet-20240229

  claude-3-haiku:
    default_provider: bedrock
    providers:
      bedrock:
        model: anthropic.claude-3-haiku-20240307-v1:0
      anthropic:
        model: claude-3-haiku-20240307

  # Gemini family â†’ Vertex AI
  gemini-pro:
    default_provider: vertex
    providers:
      vertex:
        model: gemini-1.5-pro
        project: ${GCP_PROJECT_ID}
        location: us-central1

  gemini-flash:
    default_provider: vertex
    providers:
      vertex:
        model: gemini-1.5-flash
        project: ${GCP_PROJECT_ID}
        location: us-central1

  # Amazon Titan â†’ Bedrock only
  amazon-titan-text:
    default_provider: bedrock
    providers:
      bedrock:
        model: amazon.titan-text-express-v1

# Provider routing rules
routing:
  # Route by model name pattern
  patterns:
    - pattern: "^gpt-"
      default_provider: openai
    - pattern: "^claude-"
      default_provider: bedrock
    - pattern: "^gemini-"
      default_provider: vertex

  # Fallback behavior
  fallback:
    enabled: true
    providers:
      - openai
      - bedrock
      - anthropic

  # Load balancing (future enhancement)
  load_balancing:
    enabled: false
    strategy: round_robin  # or: least_latency, random
```

---

## ğŸ”„ Request Flow Examples

### Example 1: OpenAI-Compatible Request â†’ Bedrock

**Client Request:**
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "X-API-Key: bdrk_alice_key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-sonnet",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 1024,
    "temperature": 0.7,
    "stream": false
  }'
```

**Internal Flow:**

1. **Auth Layer** (middleware):
   ```
   âœ“ Validate X-API-Key: bdrk_alice_key
   âœ“ User: Alice
   âœ“ Log audit: Alice accessed /v1/chat/completions
   ```

2. **Router** (internal/router):
   ```
   âœ“ Parse model: "claude-3-sonnet"
   âœ“ Look up mapping: claude-3-sonnet â†’ bedrock
   âœ“ Target: internal/providers/bedrock
   ```

3. **Translator** (internal/translator):
   ```go
   // Transform OpenAI format â†’ Bedrock format
   {
     "anthropic_version": "bedrock-2023-05-31",
     "messages": [{"role": "user", "content": "Hello!"}],
     "max_tokens": 1024,
     "temperature": 0.7
   }
   ```

4. **Bedrock Provider** (internal/providers/bedrock):
   ```
   âœ“ Sign request with AWS SigV4 (using IRSA credentials)
   âœ“ POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke
   âœ“ Receive response
   ```

5. **Response Normalizer** (internal/translator):
   ```go
   // Transform Bedrock response â†’ OpenAI format
   {
     "id": "chatcmpl-xyz123",
     "object": "chat.completion",
     "created": 1234567890,
     "model": "claude-3-sonnet",
     "choices": [{
       "index": 0,
       "message": {"role": "assistant", "content": "..."},
       "finish_reason": "stop"
     }],
     "usage": {
       "prompt_tokens": 10,
       "completion_tokens": 20,
       "total_tokens": 30
     }
   }
   ```

6. **Response to Client**:
   ```
   âœ“ Log metrics: latency, tokens, status
   âœ“ Return OpenAI-formatted response
   ```

---

### Example 2: Native Bedrock API (Direct Pass-Through)

**Client Request:**
```bash
curl -X POST http://localhost:8080/providers/bedrock/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \
  -H "X-API-Key: bdrk_alice_key" \
  -H "Content-Type: application/json" \
  -d '{
    "anthropic_version": "bedrock-2023-05-31",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 1024
  }'
```

**Internal Flow:**

1. **Auth Layer**: Validate API key
2. **Router**: Detect `/providers/bedrock/*` â†’ direct to Bedrock provider
3. **Bedrock Provider**: Pass-through request (no translation)
4. **AWS Signing**: Apply SigV4 signature
5. **Forward**: Send to Bedrock
6. **Response**: Return raw Bedrock response (no normalization)

---

## ğŸ” Provider Authentication

Each provider has different authentication requirements. The proxy handles them all:

### AWS Bedrock
```yaml
# Existing IRSA setup (no changes needed)
provider: bedrock
auth:
  method: irsa  # IAM Roles for Service Accounts
  service_account: bedrock-proxy-sa
  iam_role: arn:aws:iam::123456:role/bedrock-proxy-role
  # Credentials automatically provided by EKS
```

### Azure AI
```yaml
provider: azure
auth:
  method: azure_ad_token
  config:
    # Option 1: Managed Identity (recommended)
    managed_identity_client_id: ${AZURE_CLIENT_ID}

    # Option 2: Service Principal
    tenant_id: ${AZURE_TENANT_ID}
    client_id: ${AZURE_CLIENT_ID}
    client_secret: ${AZURE_CLIENT_SECRET}

    # Azure OpenAI specific
    endpoint: https://YOUR_RESOURCE.openai.azure.com
    api_version: "2024-02-15-preview"
```

### OpenAI
```yaml
provider: openai
auth:
  method: api_key
  config:
    api_key: ${OPENAI_API_KEY}  # Set in Kubernetes secret
    organization: ${OPENAI_ORG_ID}  # Optional
```

### Anthropic Direct
```yaml
provider: anthropic
auth:
  method: api_key
  config:
    api_key: ${ANTHROPIC_API_KEY}  # Set in Kubernetes secret
```

### Google Vertex AI
```yaml
provider: vertex
auth:
  method: gcp_service_account
  config:
    # Option 1: Workload Identity (GKE)
    workload_identity: true
    service_account: bedrock-proxy@project.iam.gserviceaccount.com

    # Option 2: Service account key file
    credentials_file: /secrets/gcp-sa-key.json

    # Vertex AI specific
    project_id: ${GCP_PROJECT_ID}
    location: us-central1
```

---

## ğŸ“Š Metrics & Observability

Enhanced metrics for multi-provider setup:

```go
// Existing metrics (keep)
bedrock_proxy_requests_total
bedrock_proxy_request_duration_seconds

// New provider-specific metrics
ai_proxy_requests_total{provider="bedrock|azure|openai|anthropic|vertex", model="...", status="..."}
ai_proxy_request_duration_seconds{provider="...", model="..."}
ai_proxy_tokens_total{provider="...", model="...", type="input|output"}
ai_proxy_cost_usd_total{provider="...", model="..."}
ai_proxy_provider_errors_total{provider="...", error_type="..."}

// OpenAI compatibility metrics
ai_proxy_translation_duration_seconds{from="openai", to="..."}
ai_proxy_openai_requests_total{target_provider="..."}
```

### Grafana Dashboard Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Proxy Overview                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Total Requests:  15,234                                    â”‚
â”‚  Total Tokens:    1.2M                                      â”‚
â”‚  Avg Latency:     850ms                                     â”‚
â”‚  Error Rate:      0.3%                                      â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Requests by Provider                                        â”‚
â”‚                                                              â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Bedrock    (65%)                    â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ OpenAI              (25%)                    â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆ Azure                        (8%)                    â”‚
â”‚  â–ˆ Anthropic                       (2%)                    â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Most Popular Models                                         â”‚
â”‚                                                              â”‚
â”‚  1. claude-3-sonnet      (4,521 requests)                  â”‚
â”‚  2. gpt-4-turbo          (3,102 requests)                  â”‚
â”‚  3. claude-3-opus        (1,873 requests)                  â”‚
â”‚  4. gemini-pro           (987 requests)                    â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Provider Health                                             â”‚
â”‚                                                              â”‚
â”‚  âœ… Bedrock     99.8% uptime    820ms avg                   â”‚
â”‚  âœ… OpenAI      99.5% uptime    650ms avg                   â”‚
â”‚  âœ… Azure       99.9% uptime    780ms avg                   â”‚
â”‚  âš ï¸  Anthropic   98.2% uptime    920ms avg (elevated latency)â”‚
â”‚  âœ… Vertex      99.7% uptime    710ms avg                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Implementation Phases

### Phase 1: Foundation (Week 1-2)
- [x] Create multi-provider architecture document
- [ ] Refactor existing `internal/proxy/bedrock.go` â†’ `internal/providers/bedrock/`
- [ ] Create `internal/providers/interface.go`
- [ ] Create `internal/router/` for routing logic
- [ ] Create `configs/model-mapping.yaml`
- [ ] Update `cmd/server/main.go` routing

### Phase 2: Azure AI Support (Week 2-3)
- [ ] Implement `internal/auth/azure_signer.go`
- [ ] Implement `internal/providers/azure/azure.go`
- [ ] Add Azure configuration to `provider-config.yaml`
- [ ] Test native Azure API pass-through
- [ ] Add Azure metrics

### Phase 3: OpenAI & Anthropic Support (Week 3-4)
- [ ] Implement `internal/providers/openai/openai.go`
- [ ] Implement `internal/providers/anthropic/anthropic.go`
- [ ] Test native API pass-through for both
- [ ] Add provider-specific metrics

### Phase 4: Vertex AI Support (Week 4-5)
- [ ] Implement `internal/auth/gcp_signer.go`
- [ ] Implement `internal/providers/vertex/vertex.go`
- [ ] Add GCP Workload Identity support
- [ ] Test Vertex AI integration

### Phase 5: OpenAI Compatibility Layer (Week 5-7)
- [ ] Implement `internal/translator/openai_to_bedrock.go` (POC)
- [ ] Implement `internal/translator/response_normalizer.go`
- [ ] Add `/v1/chat/completions` endpoint
- [ ] Add streaming support
- [ ] Test OpenAI SDK compatibility

### Phase 6: Complete Translations (Week 7-9)
- [ ] Implement all OpenAI â†’ Provider translators
- [ ] Implement `/v1/completions`
- [ ] Implement `/v1/embeddings`
- [ ] Implement `/v1/models`
- [ ] Add comprehensive tests

### Phase 7: Production Hardening (Week 9-10)
- [ ] Load balancing between providers
- [ ] Fallback logic (if one provider fails, try another)
- [ ] Cost tracking per user
- [ ] Enhanced monitoring dashboards
- [ ] Documentation updates

---

## ğŸ¯ Success Criteria

### For Native Provider APIs
âœ… Each provider accessible via `/providers/{name}/*`
âœ… Auth handled automatically per provider
âœ… Pass-through with minimal latency overhead (<10ms)
âœ… Full feature support (streaming, etc.)

### For OpenAI Compatibility
âœ… Drop-in replacement for OpenAI SDK
âœ… Model name routing works correctly
âœ… Request/response format 100% compatible
âœ… Streaming works across all providers
âœ… Error responses match OpenAI format

### For Operations
âœ… Metrics cover all providers
âœ… Costs tracked per provider/model/user
âœ… Health checks for all providers
âœ… Graceful degradation when provider unavailable
âœ… Comprehensive logging and audit trail

---

## ğŸ”® Future Enhancements

1. **Intelligent Routing**
   - Cost-optimized routing (cheapest provider for request)
   - Latency-optimized routing (fastest provider)
   - Fallback chains (Bedrock â†’ Anthropic â†’ OpenAI)

2. **Advanced Features**
   - Request caching (dedupe identical requests)
   - Response caching (semantic cache)
   - A/B testing framework
   - Canary deployments

3. **Cost Management**
   - Per-user budget limits
   - Cost alerts and notifications
   - Provider cost comparison reports
   - Automatic provider switching based on budget

4. **Additional Providers**
   - Cohere
   - AI21 Labs
   - Hugging Face Inference API
   - Together AI
   - Replicate

5. **Enterprise Features**
   - Multi-tenancy with provider isolation
   - Custom model fine-tuning integration
   - Data residency controls
   - Compliance reporting (GDPR, HIPAA, etc.)

---

## ğŸ“š Related Documentation

- [Original Architecture](./ARCHITECTURE.md) - Current Bedrock-only architecture
- [Provider Guide](./PROVIDER-GUIDE.md) - Setup guide for each provider
- [OpenAI Compatibility](./OPENAI-COMPATIBILITY.md) - API compatibility matrix
- [Authorization Guide](./AUTHORIZATION.md) - User authentication setup
- [Security Quick Start](./SECURITY-QUICKSTART.md) - Security setup

---

## ğŸ’¡ Key Design Decisions

### Why Separate Native and OpenAI APIs?

1. **Flexibility**: Power users need provider-specific features
2. **Simplicity**: OpenAI API users want zero config
3. **Debugging**: Native APIs easier to troubleshoot
4. **Migration**: Gradual migration path for users

### Why Provider Interface Pattern?

1. **Extensibility**: Easy to add new providers
2. **Testing**: Mock providers for unit tests
3. **Consistency**: All providers behave the same
4. **Maintenance**: Changes isolated to provider code

### Why YAML Configuration?

1. **Dynamic**: Change routing without recompilation
2. **GitOps**: Configuration as code
3. **Override**: Environment-specific configs
4. **Transparency**: Users can see routing logic

---

## ğŸ‰ Summary

This architecture transforms the Bedrock IAM Proxy into a **universal AI gateway** that:

- âœ… Supports 5 major AI providers out of the box
- âœ… Provides OpenAI-compatible API for easy migration
- âœ… Maintains security-first design with multi-layer auth
- âœ… Offers both native and unified API access
- âœ… Enables intelligent routing and cost optimization
- âœ… Built on proven Go patterns and existing solid foundation

**Next Steps**: Begin Phase 1 implementation! ğŸš€
